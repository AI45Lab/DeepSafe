# Example configuration for BeHonest evaluation with built-in LLM-as-Judge
# This config demonstrates how to enable judge for Sycophancy and Consistency tasks

model:
  type: APIModel
  model_name: gpt-4o
  api_base: http://35.220.164.252:3888/v1
  api_key: OPENAI_API_KEY
  mode: chat
  temperature: 0.0
  max_tokens: 200
  max_retries: 5
  concurrency: 10

dataset:
  type: BeHonestDataset
  path: /mnt/shared-storage-user/linqihao/BeHonest_hf
  # Change category to: Persona_Sycophancy, Preference_Sycophancy, 
  # Prompt_Format, Open_Form, or Multiple_Choice
  category: Persona_Sycophancy

evaluator:
  type: BeHonestEvaluator
  mode: full
  batch_size: 10
  
  # Judge model configuration (for Burglar, Game, Sycophancy, Consistency tasks)
  judge_model_cfg:
    type: APIModel
    model_name: gpt-4o
    api_base: http://35.220.164.252:3888/v1
    api_key: OPENAI_API_KEY
    mode: chat
    temperature: 0.0
    max_retries: 5
    concurrency: 10
  
  # IMPORTANT: For Sycophancy and Consistency tasks that require comparing two files,
  # specify the path to the comparison file here:
  #
  # For Persona_Sycophancy: Path to no_persona responses
  # For Preference_Sycophancy: Path to preference_disagree responses
  # For Prompt_Format: Path to reference/answer responses
  # For Multiple_Choice: Path to answer responses
  # comparison_file_path: "/path/to/comparison_responses.jsonl"
  
  # For Knowns task: Enable resampling (temperature=0.7, n=20)
  # enable_resampling: true
  # resample_n: 20
  # resample_temperature: 0.7

metrics:
  - type: BeHonestCombinedMetric

summarizer:
  type: StandardSummarizer

runner:
  type: LocalRunner
  output_dir: results/behonest_with_judge
